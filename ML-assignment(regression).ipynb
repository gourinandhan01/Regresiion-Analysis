{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc4315e0-6725-4ee9-b242-9ca6a53bc4c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Information:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20640 entries, 0 to 20639\n",
      "Data columns (total 9 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   MedInc       20640 non-null  float64\n",
      " 1   HouseAge     20640 non-null  float64\n",
      " 2   AveRooms     20640 non-null  float64\n",
      " 3   AveBedrms    20640 non-null  float64\n",
      " 4   Population   20640 non-null  float64\n",
      " 5   AveOccup     20640 non-null  float64\n",
      " 6   Latitude     20640 non-null  float64\n",
      " 7   Longitude    20640 non-null  float64\n",
      " 8   MedHouseVal  20640 non-null  float64\n",
      "dtypes: float64(9)\n",
      "memory usage: 1.4 MB\n",
      "None\n",
      "\n",
      "Dataset Description:\n",
      "             MedInc      HouseAge      AveRooms     AveBedrms    Population  \\\n",
      "count  20640.000000  20640.000000  20640.000000  20640.000000  20640.000000   \n",
      "mean       3.870671     28.639486      5.429000      1.096675   1425.476744   \n",
      "std        1.899822     12.585558      2.474173      0.473911   1132.462122   \n",
      "min        0.499900      1.000000      0.846154      0.333333      3.000000   \n",
      "25%        2.563400     18.000000      4.440716      1.006079    787.000000   \n",
      "50%        3.534800     29.000000      5.229129      1.048780   1166.000000   \n",
      "75%        4.743250     37.000000      6.052381      1.099526   1725.000000   \n",
      "max       15.000100     52.000000    141.909091     34.066667  35682.000000   \n",
      "\n",
      "           AveOccup      Latitude     Longitude   MedHouseVal  \n",
      "count  20640.000000  20640.000000  20640.000000  20640.000000  \n",
      "mean       3.070655     35.631861   -119.569704      2.068558  \n",
      "std       10.386050      2.135952      2.003532      1.153956  \n",
      "min        0.692308     32.540000   -124.350000      0.149990  \n",
      "25%        2.429741     33.930000   -121.800000      1.196000  \n",
      "50%        2.818116     34.260000   -118.490000      1.797000  \n",
      "75%        3.282261     37.710000   -118.010000      2.647250  \n",
      "max     1243.333333     41.950000   -114.310000      5.000010  \n",
      "\n",
      "Missing values:\n",
      "MedInc         0\n",
      "HouseAge       0\n",
      "AveRooms       0\n",
      "AveBedrms      0\n",
      "Population     0\n",
      "AveOccup       0\n",
      "Latitude       0\n",
      "Longitude      0\n",
      "MedHouseVal    0\n",
      "dtype: int64\n",
      "\n",
      "Variable Verification:\n",
      "X_train_scaled: <class 'numpy.ndarray'>, Shape: (16512, 8)\n",
      "y_train: <class 'pandas.core.series.Series'>, Shape: (16512,)\n",
      "X_test_scaled: <class 'numpy.ndarray'>, Shape: (4128, 8)\n",
      "y_test: <class 'pandas.core.series.Series'>, Shape: (4128,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the dataset\n",
    "data = fetch_california_housing(as_frame=True)\n",
    "df = data.frame\n",
    "\n",
    "# Check basic information\n",
    "print(\"Dataset Information:\")\n",
    "print(df.info())\n",
    "\n",
    "# Describe dataset statistics\n",
    "print(\"\\nDataset Description:\")\n",
    "print(df.describe())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Separate features and target variable\n",
    "X = df.drop(\"MedHouseVal\", axis=1)\n",
    "y = df[\"MedHouseVal\"]\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Verify variable definitions and print their types and shapes\n",
    "print(\"\\nVariable Verification:\")\n",
    "print(f\"X_train_scaled: {type(X_train_scaled)}, Shape: {X_train_scaled.shape}\")\n",
    "print(f\"y_train: {type(y_train)}, Shape: {y_train.shape}\")\n",
    "print(f\"X_test_scaled: {type(X_test_scaled)}, Shape: {X_test_scaled.shape}\")\n",
    "print(f\"y_test: {type(y_test)}, Shape: {y_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58cfeb9-6a5d-4f68-a32b-ba8cda176084",
   "metadata": {},
   "source": [
    "### **Preprocessing Steps and Justification**\r\n",
    "\r\n",
    "1. **Loading the Dataset**:\r\n",
    "   - We loaded the California Housing dataset using `fetch_california_housing` from `sklearn.datasets`. This dataset contains multiple features about different California districts, such as latitude, longitude, population, household statistics, and the target variable, `MedHouseVal`, which represents the median house value.\r\n",
    "   \r\n",
    "   **Justification**: This step is essential to obtain the dataset, which we will use to train and evaluate our regression models.\r\n",
    "\r\n",
    "2. **Exploring the Dataset**:\r\n",
    "   - We used `df.info()` to display the data types and the number of non-null entries in each column, and `df.describe()` to get summary statistics of the numeric features.\r\n",
    "   \r\n",
    "   **Justification**: Understanding the data structure and getting a statistical summary is crucial to detect potential issues such as missing data, outliers, or extreme values that could impact the modeling process.\r\n",
    "\r\n",
    "3. **Checking for Missing Values**:\r\n",
    "   - We checked for missing values using `df.isnull().sum()`.\r\n",
    "   \r\n",
    "   **Justification**: Missing data can cause problems for machine learning algorithms, leading to biased or incorrect model predictions. Identifying missing values early on helps decide the best approach for handling them (e.g., imputation or removal).\r\n",
    "\r\n",
    "4. **Feature and Target Variable Separation**:\r\n",
    "   - We separated the features (independent variables) from the target variable (`MedHouseVal`) by dropping the target column from the feature set.\r\n",
    "   \r\n",
    "   **Justification**: In supervised learning, we need to separate the features (input variables) from the target variable (output). This separation allows the model to learn from the features and predict the target.\r\n",
    "\r\n",
    "5. **Splitting the Data into Training and Testing Sets**:\r\n",
    "   - We used `train_test_split` from `sklearn.model_selection` to split the data into training and testing sets. 80% of the data was used for training, and 20% was set aside for testing.\r\n",
    "   \r\n",
    "   **Justification**: Splitting the dataset into training and testing sets ensures that the model can be trained on one subset of the data and tested on another unseen subset. This helps to evaluate the model's performance and generalizability.\r\n",
    "\r\n",
    "6. **Feature Scaling**:\r\n",
    "   - We scaled the features using `StandardScaler` to standardize the training and testing data. This transformation adjusts the features to have a mean of 0 and a standard deviation of 1.\r\n",
    "   \r\n",
    "   **Justification**: Many machine learning algorithms, especially those based on gradient descent, are sensitive to the scale of the features. Scaling the features ensures that no single feature dominates the model due to differences in magnitude. It also speeds up the convergence of gradient-based algorithms.\r\n",
    "\r\n",
    "7. **Variable Verification**:\r\n",
    "   - After scaling, we verified the types and shapes of the resulting variables (`X_train_scaled`, `X_test_scaled`, `y_train`, `y_test`) to ensure that the data was processed correctly.\r\n",
    "   \r\n",
    "   **Justification**: Verifying the data after preprocessing ensures that the dataset is ready for model training. It helps catch any issues such as missing variables or incorrect transformations that could affect the model's performance.\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee4cc51-cd45-4fc8-8d90-e6e9a0999069",
   "metadata": {},
   "source": [
    "### 1. Linear Regression ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d90086b4-af97-4ec1-9156-fff34f7113c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression Performance:\n",
      "Mean Squared Error (MSE): 0.5558915986952441\n",
      "Mean Absolute Error (MAE): 0.5332001304956566\n",
      "R-squared (R²): 0.575787706032451\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Initialize and train the model\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_lr = lr_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "mse_lr = mean_squared_error(y_test, y_pred_lr)\n",
    "mae_lr = mean_absolute_error(y_test, y_pred_lr)\n",
    "r2_lr = r2_score(y_test, y_pred_lr)\n",
    "\n",
    "# Display results\n",
    "print(\"Linear Regression Performance:\")\n",
    "print(f\"Mean Squared Error (MSE): {mse_lr}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae_lr}\")\n",
    "print(f\"R-squared (R²): {r2_lr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3cc1546-4771-47f7-b8ff-eddf298e5230",
   "metadata": {},
   "source": [
    "### 2. Decision Tree Regressor ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "96d9018a-437e-4d16-936b-9a178e793f14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Decision Tree Regressor Performance:\n",
      "Mean Squared Error (MSE): 0.49396854311945243\n",
      "Mean Absolute Error (MAE): 0.45390448401162786\n",
      "R-squared (R²): 0.6230424613065773\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Initialize and train the model\n",
    "dt_model = DecisionTreeRegressor(random_state=42)\n",
    "dt_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_dt = dt_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "mse_dt = mean_squared_error(y_test, y_pred_dt)\n",
    "mae_dt = mean_absolute_error(y_test, y_pred_dt)\n",
    "r2_dt = r2_score(y_test, y_pred_dt)\n",
    "\n",
    "# Display results\n",
    "print(\"\\nDecision Tree Regressor Performance:\")\n",
    "print(f\"Mean Squared Error (MSE): {mse_dt}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae_dt}\")\n",
    "print(f\"R-squared (R²): {r2_dt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5db96a9-74b1-4bb6-8ffa-d496b1a23921",
   "metadata": {},
   "source": [
    "### 3. Random Forest Regressor ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "926e9ac7-4a4d-4673-9b61-6a313fcc352d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Random Forest Regressor Performance:\n",
      "Mean Squared Error (MSE): 0.255169737347244\n",
      "Mean Absolute Error (MAE): 0.3274252027374032\n",
      "R-squared (R²): 0.8052747336256919\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Initialize and train the model\n",
    "rf_model = RandomForestRegressor(random_state=42, n_estimators=100)\n",
    "rf_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_rf = rf_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
    "mae_rf = mean_absolute_error(y_test, y_pred_rf)\n",
    "r2_rf = r2_score(y_test, y_pred_rf)\n",
    "\n",
    "# Display results\n",
    "print(\"\\nRandom Forest Regressor Performance:\")\n",
    "print(f\"Mean Squared Error (MSE): {mse_rf}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae_rf}\")\n",
    "print(f\"R-squared (R²): {r2_rf}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b28619-dd49-4526-b6ca-7128d15f2e0f",
   "metadata": {},
   "source": [
    "### 4. Gradient Boosting Regressor ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c227d89d-dd58-4877-98a8-120edad5d0f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gradient Boosting Regressor Performance:\n",
      "Mean Squared Error (MSE): 0.29399901242474274\n",
      "Mean Absolute Error (MAE): 0.37165044848436773\n",
      "R-squared (R²): 0.7756433164710084\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Initialize and train the model\n",
    "gb_model = GradientBoostingRegressor(random_state=42)\n",
    "gb_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_gb = gb_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "mse_gb = mean_squared_error(y_test, y_pred_gb)\n",
    "mae_gb = mean_absolute_error(y_test, y_pred_gb)\n",
    "r2_gb = r2_score(y_test, y_pred_gb)\n",
    "\n",
    "# Display results\n",
    "print(\"\\nGradient Boosting Regressor Performance:\")\n",
    "print(f\"Mean Squared Error (MSE): {mse_gb}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae_gb}\")\n",
    "print(f\"R-squared (R²): {r2_gb}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8464c4d6-d1d1-47d5-8e9a-23837b9b413d",
   "metadata": {},
   "source": [
    "### 5. Support Vector Regressor (SVR) ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "207104e2-893c-413b-a736-70f73f6d147a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Support Vector Regressor Performance:\n",
      "Mean Squared Error (MSE): 0.3570040319338641\n",
      "Mean Absolute Error (MAE): 0.3985990769520539\n",
      "R-squared (R²): 0.7275628923016779\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "# Initialize and train the model\n",
    "svr_model = SVR(kernel='rbf')\n",
    "svr_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_svr = svr_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "mse_svr = mean_squared_error(y_test, y_pred_svr)\n",
    "mae_svr = mean_absolute_error(y_test, y_pred_svr)\n",
    "r2_svr = r2_score(y_test, y_pred_svr)\n",
    "\n",
    "# Display results\n",
    "print(\"\\nSupport Vector Regressor Performance:\")\n",
    "print(f\"Mean Squared Error (MSE): {mse_svr}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae_svr}\")\n",
    "print(f\"R-squared (R²): {r2_svr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296169e0-081a-4559-8386-b1c6ae067ab4",
   "metadata": {},
   "source": [
    "### **Regression Algorithms: Explanation and Suitability**\r\n",
    "\r\n",
    "1. **Linear Regression**:\r\n",
    "   - **How it Works**: Linear Regression models the relationship between the target variable and one or more independent variables by fitting a straight line (in higher dimensions, a hyperplane) that minimizes the sum of squared residuals.\r\n",
    "   - **Suitability for this Dataset**: Linear Regression is suitable for datasets where there is a linear relationship between the features and the target variable. In this dataset, it provides a baseline model to evaluate how well simple linear relationships capture the variability in median house values.\r\n",
    "\r\n",
    "2. **Decision Tree Regressor**:\r\n",
    "   - **How it Works**: Decision Tree Regressor splits the dataset into subsets based on feature thresholds, creating a tree structure where each leaf represents a predicted value. It minimizes the variance within each subset.\r\n",
    "   - **Suitability for this Dataset**: This algorithm is non-parametric and can capture complex, non-linear relationships between features and the target variable, which might be present in housing data.\r\n",
    "\r\n",
    "3. **Random Forest Regressor**:\r\n",
    "   - **How it Works**: Random Forest is an ensemble method that builds multiple decision trees using random subsets of the data and features. It aggregates their predictions to improve accuracy and reduce overfitting.\r\n",
    "   - **Suitability for this Dataset**: The ensemble approach of Random Forest is effective for datasets with high variability, like this housing dataset, as it reduces overfitting and provides robust predictions.\r\n",
    "\r\n",
    "4. **Gradient Boosting Regressor**:\r\n",
    "   - **How it Works**: Gradient Boosting builds a series of decision trees sequentially, where each tree tries to correct the errors of the previous one. It uses gradient descent to optimize a loss function.\r\n",
    "   - **Suitability for this Dataset**: Gradient Boosting is well-suited for datasets where high prediction accuracy is required. It can model complex relationships in the data effectively.\r\n",
    "\r\n",
    "5. **Support Vector Regressor (SVR)**:\r\n",
    "   - **How it Works**: SVR attempts to fit the best hyperplane within a margin of tolerance for error, optimizing a loss function that ignores small deviations from the target value.\r\n",
    "   - **Suitability for this Dataset**: SVR works well when the dataset has non-linear relationships and is sensitive to feature scaling, making it effective after standardization. However, it may struggle with larger datasets due to compuent for accurate predictions in real-world scenarios.\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa44b75-5515-45b9-a998-842ab166fddb",
   "metadata": {},
   "source": [
    "### **Model Evaluation and Comparison**\n",
    "\n",
    "We evaluated the performance of each regression algorithm using the following metrics:\n",
    "\n",
    "1. **Mean Squared Error (MSE)**: Measures the average squared difference between predicted and actual values. Lower values indicate better performance.\n",
    "2. **Mean Absolute Error (MAE)**: Measures the average absolute difference between predicted and actual values. Lower values indicate better performance.\n",
    "3. **R-squared Score (R²)**: Indicates the proportion of variance in the target variable explained by the model. Values closer to 1 indicate better performance.\n",
    "\n",
    "| Algorithm                | MSE       | MAE       | R²         |\n",
    "|--------------------------|-----------|-----------|------------|\n",
    "| Linear Regression        | 0.5559    | 0.5332    | 0.5758     |\n",
    "| Decision Tree Regressor  | 0.4940    | 0.4539    | 0.6230     |\n",
    "| Random Forest Regressor  | 0.2552    | 0.3274    | 0.8053     |\n",
    "| Gradient Boosting Regressor | 0.2940 | 0.3717    | 0.7756     |\n",
    "| Support Vector Regressor | 0.3570    | 0.3986    | 0.7276     |\n",
    "\n",
    "---\n",
    "\n",
    "### **Best-Performing Algorithm**\n",
    "- **Algorithm**: Random Forest Regressor\n",
    "- **Justification**: \n",
    "  - It achieved the lowest Mean Squared Error (MSE) and Mean Absolute Error (MAE), indicating that its predictions are closest to the actual values.\n",
    "  - It also obtained the highest R-squared (R²) score of 0.8053, showing that it explains the largest proportion of variance in the target variable. \n",
    "  - The ensemble nature of Random Forest helps it effectively model complex relationships and reduces overfitting.\n",
    "\n",
    "### **Worst-Performing Algorithm**\n",
    "- **Algorithm**: Linear Regression\n",
    "- **Reasoning**: \n",
    "  - Linear Regression assumes a linear relationship between features and the target variable, which is a significant limitation for this dataset.\n",
    "  - It had the highest MSE and MAE among all models, and its R-squared score of 0.5758 was the lowest, indicating it explains the least variance in the target variable.\n",
    "\n",
    "---\n",
    "\n",
    "### **Conclusion**\n",
    "The results highlight the importance of using advanced algorithms like Random Forest or Gradient Boosting for datasets with complex relationships. While simpler models like Linear Regression provide a good baseline, they fail to capture the intricate patterns in the data. Random Forest Regressor outperformed all other models, making it the best choice for this dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e999a0ca-dd88-4957-a951-7d52968c8dcb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
